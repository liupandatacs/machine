# 原始数据处理

##  数据来源

- **确诊数据**由儿保提供，主要来源于浙江省儿保，合肥，泉州，包含22种疾病，总共822例。疾病情况如下表：

  

  |              **疾病**               | **数目** |
  | :---------------------------------: | :------: |
  |             **轻度PKU**             |  **72**  |
  |             **经典PKU**             | **107**  |
  |            **BH4缺乏症**            |  **10**  |
  |               **PCD**               | **127**  |
  |       **脂肪酸代谢异常携带**        |  **45**  |
  |              **SCAD**               |  **64**  |
  | **异丁酰辅酶****A****脱氢酶缺乏症** |  **20**  |
  |         **甲基丙二酸血症**          |  **59**  |
  |            **丙酸血症**             |  **12**  |
  |              **NICCD**              |  **33**  |
  |              **MCCD**               |  **51**  |
  |       **有机酸代谢异常携带**        |  **23**  |
  |               **MET**               |  **27**  |
  |           **异戊酸血症**            |  **15**  |
  |      **2-甲基丁酰甘氨酸尿症**       |  **20**  |
  |    **多种酰基辅酶脱氢酶缺乏症**     |  **11**  |
  |              **VLCAD**              |  **17**  |
  |              **MCADD**              |  **15**  |
  |              **CPT1**               |  **10**  |
  |            **枫糖尿症**             |  **8**   |
  |         **瓜氨酸血症(ASS)**         |  **9**   |
  |           **脯氨酸血症**            |  **11**  |
  |          **戊二酸血症I型**          |  **16**  |
  |              **总数**               | **782**  |

- **筛查数据** 来自浙江省儿保所有的数据，数据目前在服务器上，数据量比较大

## 数据处理

###  原始数据处理.py

- **删除无用的列名，统一表名**

  ```python
  shaicha_data['NAME'] = 'erbao'
  # 删除无用的列
  del shaicha_data['BARCODESTRING']
  del shaicha_data['Unnamed: 0']
  del shaicha_data['hospital']
  ```

- **增加采血间隔字段，采血间隔字段对指标影响比较大**

```python
# 增加'AGE_DAY'采血时间间隔
confirmed_data['COLLECTDATE'] = confirmed_data['COLLECTDATE'].astype(str)
confirmed_data['BIRTHDAY'] = confirmed_data['BIRTHDAY'].astype(str)
confirmed_data['MSMS_LAB_DATE'] = confirmed_data['MSMS_LAB_DATE'].astype(str)

confirmed_data['COLLECTDATE'] = pd.to_datetime(confirmed_data['COLLECTDATE'], errors = 'coerce')
confirmed_data['BIRTHDAY'] = pd.to_datetime(confirmed_data['BIRTHDAY'],errors = 'coerce')
confirmed_data['MSMS_LAB_DATE'] = pd.to_datetime(confirmed_data['MSMS_LAB_DATE'],errors = 'coerce')

confirmed_data['AGE_DAY'] = (confirmed_data['COLLECTDATE'] - confirmed_data['BIRTHDAY']).dt.days
```

- **去除重复数据**

  ```python
  # 通过相同字段删除样本中的重复样本
  ziduan = ['ALA','ARG','LEU','GLY','MET']
  confirmed_data = confirmed_data.drop_duplicates(ziduan)
  shaicha_data = shaicha_data.drop_duplicates(ziduan)
  print(len(shaicha_data),len(confirmed_data))
  
  # 合并数据
  alldata = pd.concat([confirmed_data,shaicha_data])
  alldata['CHONGEFU'] =  alldata[ziduan].duplicated(keep=False)
  
  # 去重，挑取样本中的唯一样本，和确诊样本
  alldata_quchong = alldata[(alldata['CHONGEFU']==False)|(alldata['疾病'].notnull())] 
  
  ```

- **挑选筛查数据**

  ```python
  notconfirmed = alldata_quchong[alldata_quchong['疾病'].isnull()]
  # 截取儿保2015-2018的数据
  notconfirmed = notconfirmed[(notconfirmed['MSMS_LAB_DATE']>str(2015))&(notconfirmed['MSMS_LAB_DATE']<str(2019))] 
  # 截取儿保数据中体重和孕周都不为空的数据
  notconfirmed = notconfirmed[(-notconfirmed['WEIGHT'].isnull())& (-notconfirmed['GESTATIONAL_WEEKS_WEEKS'].isnull())]
  print(len(notconfirmed))==1887986
  ```

### data_MOM.py

- **填充空值**

  ​	填充指标空值，在确诊样本中各样本根据疾病进行分类，然后根据相似的样本的中值填充指标空值

  ​	填充采血间隔日期，空值从随机选择**3-7天**

  ​	填充体重，正常样本的体重**均值**

  ​	填充孕周，正常样本的孕周的**众数**

  ​	填充性别，随机选择男，女填充

- **性别处理**，将男的映射成1，女的映射成0

- **数据调整**

  - 主要将上文的到的数据通过**采血间隔—体重**（**median_key_het2500.csv，median_key_lt2500.csv**）分析结果调整MSMS指标，使得在不同采血间隔和不同体重下的样本除以其所对应的中值（MOM化处理）

  - **median_key_het2500.csv** ：体重大于等于**2500**克的样本随着采血间隔变化对MSMS指标的影响

  - **median_key_lt2500.csv** ：体重小于**2500**克的样本随着采血间隔变化对MSMS指标的影响

  - ```python
    # 数据调整
    #确诊数据调整
    for i in msms_feature:
        print(i)
        for j in range(2,30):
            if j<=28:
                confirmed[i][(confirmed['AGE_DAY']==j)&(confirmed['WEIGHT']<2500)]=confirmed[i][(confirmed['AGE_DAY']==j)&(confirmed['WEIGHT']<2500)].apply(lambda x: float(x)/float(median_key_lt2500[i][median_key_lt2500['采血间隔']==j]))
                confirmed[i][(confirmed['AGE_DAY']==j)&(confirmed['WEIGHT']>=2500)]=confirmed[i][(confirmed['AGE_DAY']==j)&(confirmed['WEIGHT']>=2500)].apply(lambda x: float(x)/float(median_key_mt2500[i][median_key_mt2500['采血间隔']==j]))
    
            else:
                confirmed[i][(confirmed['AGE_DAY']>=j)&(confirmed['WEIGHT']<2500)]=confirmed[i][(confirmed['AGE_DAY']>=j)&(confirmed['WEIGHT']<2500)].apply(lambda x: float(x)/float(median_key_lt2500[i][median_key_lt2500['采血间隔']==j]))
                confirmed[i][(confirmed['AGE_DAY']>=j)&(confirmed['WEIGHT']>=2500)]=confirmed[i][(confirmed['AGE_DAY']>=j)&(confirmed['WEIGHT']>=2500)].apply(lambda x: float(x)/float(median_key_mt2500[i][median_key_mt2500['采血间隔']==j]))
    
     # 正常数据
    for i in msms_feature:
        print(i)
        for j in range(2,30):
            if j<=28:
                notconfirmed_sample[i][(notconfirmed_sample['AGE_DAY']==j)&(notconfirmed_sample['WEIGHT']<2500)]=notconfirmed_sample[i][(notconfirmed_sample['AGE_DAY']==j)&(notconfirmed_sample['WEIGHT']<2500)].apply(lambda x: float(x)/float(median_key_lt2500[i][median_key_lt2500['采血间隔']==j]))
                notconfirmed_sample[i][(notconfirmed_sample['AGE_DAY']==j)&(notconfirmed_sample['WEIGHT']>=2500)]=notconfirmed_sample[i][(notconfirmed_sample['AGE_DAY']==j)&(notconfirmed_sample['WEIGHT']>=2500)].apply(lambda x: float(x)/float(median_key_mt2500[i][median_key_mt2500['采血间隔']==j]))
    
            else:
                notconfirmed_sample[i][(notconfirmed_sample['AGE_DAY']>=j)&(notconfirmed_sample['WEIGHT']<2500)]=notconfirmed_sample[i][(notconfirmed_sample['AGE_DAY']>=j)&(notconfirmed_sample['WEIGHT']<2500)].apply(lambda x: float(x)/float(median_key_lt2500[i][median_key_lt2500['采血间隔']==j]))
                notconfirmed_sample[i][(notconfirmed_sample['AGE_DAY']>=j)&(notconfirmed_sample['WEIGHT']>=2500)]=notconfirmed_sample[i][(notconfirmed_sample['AGE_DAY']>=j)&(notconfirmed_sample['WEIGHT']>=2500)].apply(lambda x: float(x)/float(median_key_mt2500[i][median_key_mt2500['采血间隔']==j]))
    
    ```


  ### 数据处理

  将msms指标中为0 的数据换为0.005

  ### 构建比值特征

  msms_feature = [ 'ALA',
         'ARG', 'CIT', 'GLY', 'LEU', 'MET', 'ORN', 'PHE', 'PRO', 'SA', 'TYR',
         'VAL', 'C0', 'C10', 'C10:1', 'C10:2', 'C12', 'C12:1', 'C14', 'C14:1',
         'C14:2', 'C14OH', 'C16', 'C16:1', 'C16:1OH', 'C16OH', 'C18', 'C18:1',
         'C18:1OH', 'C18:2', 'C18OH', 'C2', 'C3', 'C3DC+C4OH', 'C4', 'C4DC+C5OH',
         'C5', 'C5:1', 'C5DC+C6OH', 'C6', 'C6DC', 'C8', 'C8:1','consult']

  比值特征主要是在msms指标中构建，两两进行比值处理，在其中加入一个对照指标**consult**，对照指标中的数值随机产生，对预测结果应该毫无意义，主要作用是为了在重要特征选择时剔除与其重要性相似的指标

  ### 特征选择

  整体思想是通过欠采样的方法，通过随机森林来选择重要特征

  1 数据特征是正常样本比较多，确诊样本比较少，在这种情况下，随机抽取正常样本100次，每次抽取的数量为确诊样本的1.2倍，保证正负样本比较平衡

  2 通过每次抽取的正常样本与用来训练的确诊样本进行

  ## FM （Factorization Machine）

  ### 背景

  FM主要目标是：<font color="red">解决数据稀疏情况下，特征怎样组合的问题</font>，是一种基于LR模型的高效的学习特征之间的相互关系，最大的特点是对于稀疏的数据具有很好的学习能力。

  ### 优点

  FM可以在<font color="red">非常稀疏的数据</font>中进行合理的参数估计，而SVM做不到

  FM的<font color='red'>复杂度是线性的</font>，优化效果很好，而且不需要像SVM一样依赖于支持向量

  FM是一个<font color='red'>通用模型</font>，它可以用于任何特征为实值的情况。

  ### FM 模型

  在一般的线性模型中，是各个特征独立考虑的，没有考虑到特征与特征之间的相互关系。但实际上，大量的特征之间是有关联的。最简单的以电商为例，一般女性用户看化妆品服装之类的广告比较多，而男性更青睐各种球类装备。那很明显，<font color='red'>女性这个特征</font>与化妆品类商品有很大的关联性，<font color=red>男性这个特征</font>与球类装备的关联性更为密切。如果我们能将这些有关联的特征找出来，显然很有意义。
$$
  y(x)=\omega_{0}+\sum_{i=1}^{n}\omega_{i}x_{i}+\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}\omega_{ij}x_{i}x_{j}
$$
  从公式(1)可以看出，组合特征的参数一共有 *n*(*n*−1)/2个，任意两个**参数**都是独立的。然而，在数据稀疏性普遍存在的实际应用场景中，二次项参数的训练是很困难的。其原因是，每个参数 *w**i**j*的训练需要大量 xi和 xj*都非零的样本；由于样本数据本来就比较稀疏，满足xi 和 xj 都非零”的样本将会非常少。训练样本的不足，很容易导致参数 *w**i**j* 不准确，最终将严重影响模型的性能。

  

  一般的线性模型为（n为特征维度）：
$$
  y=\omega_{0}+\sum_{i=1}^{n}\omega_{i}x_{i}
$$

​     对于**度为2**的因子分解机模型为：
$$
y=\omega_{0}+\sum_{i=1}^{n}\omega_{i}x_{i}+\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}<v_{i},v_{j}>x_{i}x_{j}
$$

$$
v\in{R}^{n,k},<V_{i},V_{j}>表示的是两个大小为k的向量之间的点积：\\\\
<V_{i},V_{j}>=\sum_{f=1}^{k}v_{i,f}\cdot{v_{j,f}}
$$

从上式可以看出二项式的参数数量由原来的*n*(*n*−1)2个减少为nk，远少于多项式模型的参数数量。另外，参数因子化使得xh\*xi的参数和xh\*xj的参数不再相互独立，因为有了x\*h特征关联。因此我们可以在样本系数的情况下相对合理地估计FM的二次项参数。 
具体来说，xh\*xi和xh\*xj的系数分别为<vh,vi>,<vh,vj>,它们之间的共同项vh,因此所有包含xh的非零组合特征的样本都可以用来学习隐向量vh，很大程度上避免了数据稀疏性造成的影响。

这是因为在稀疏条件下，这样的表示方法打破了特征的独立性，能够更好地挖掘特征之间的相关性。以上述电影为例，我们要估计用户A和电影ST的关系w(A&ST)以更好地预测y，如果是简单地考虑特征之间的共现情况来估计w(A&ST)，从已有的训练样本来看，这两者并没有共现，因此学习出来的w(A&ST)=0。而实际上，A和ST应该是存在某种联系的，从用户角度来看，A和B都看过SW，而B还看过ST，说明A也可能喜欢ST，说明A很有可能也喜欢ST。而通过向量v来表示用户和电影，任意两两之间的交互都会影响v的更新，从前面举的例子就可以看过，A和B看过SW，这样的交互关系就会导致v(ST)的学习更新，因此通过向量v的学习方式能够更好的挖掘特征间的相互关系，尤其在稀疏条件下。

与线性模型相比，FM模型就多了后面特征组合的部分。
$$
\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}(V_{i}^{T}V_{j})x_{i}x_{j}=\frac{1}{2}\lbrace\sum_{i=1}^{n}\sum_{j=1}^{n}(V_{i}^{T}V_{j})x_{i}x_{j}-\sum_{i=1}^{n}(V_{i}^{T}V_{i})x_{i}x_{i}
\rbrace\\\\
\qquad\qquad\qquad\qquad =\frac{1}{2}\lbrace
\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{l=1}^{k}v_{il}v_{jl}x_{i}x_{j}-\sum_{i=1}^{n}\sum_{l=1}^{k}v_{il}^{2}x_{i}^{2}
\rbrace\\\\
\qquad\qquad\qquad\qquad =\frac{1}{2}\sum_{l=1}^{k}\lbrace
\sum_{i=1}^n(v_{il}x_{i})\sum_{j=1}^{n}(v_{jl}x_{j})-\sum_{i=1}^{n}v_{il}^{2}x_{i}^{2}
\rbrace\\\\
\qquad\qquad\quad =\frac{1}{2}\sum_{l=1}^{k}\lbrace
(\sum_{i=1}^{n}(v_{il}x_{i}))^{2}-\sum_{i=1}^{n}v_{il}^{2}x_{i}^{2}
\rbrace
$$

* 1 在该算法进行特征组合挑选时，当**迭代次数**和**隐向量**不同时，会导致预测结果会有波动，导致一些不同的确诊样本漏筛
* 2 确诊样本比较少，与不同训练集进行训练时得到的模型，对测试集进行预测时总是漏筛两个泉州的样本，其它样本只是预测概率有所不同，说明在确诊样本不变的情况下，各个特征学习到的**隐向量**变化或者说是影响变小
* 3 当训练样本中的正常样本改变后，使用相同的参数进行训练，得到的模型不同，在测试集上的表现会有变化，即预测概率有所变化，说明模型学到的侧重点有所变化，需要将这些侧重点整理在一起
* 4 该算法在所进行的**特征组合**是计算特征之间的**乘积** 
* 5 对于样本量少，所具有的信息比较少，当特征比较多时，所对应学习到的隐向量就比较小
* 6 将不同的正常样本与相同的确诊样本进行合并训练，进行多次实验，每次实验得到的组合特征权重与**参照特征**进行比较，参照特征是指：与consult进行组合的特征，而consult是自定义的随机数。如果组合特征的权重的绝对值比对照特征权重大，说明该组合特征有一定的意义，将其保留，将每次实验的相同组合特征的权重进行加和。最后将权重从大到小排序，得到特征的重要性

### 问题

- 当前做的模型总是出现过拟合问题

  1 由于训练样本比较少，直接使用Embedding时，相当于将每个特征都进行了映射或者压缩，在样本量比较少，特征比较多的情况下，很难找到特征的真正关联，很容易出现过拟合现象，导致组合乘积的效果作用不大

  2 由于样本量比较少，特征比较多，所以将特征分为几部分，套用随机森林的想法，随机取训练样本，随机取部分特征，进行模型训练，计算每个特征的权重，最后进行统计，得到的特征差异度也不强